{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Bag of Words\n",
    "\n",
    "-------------------------------------\n",
    "\n",
    "**在此示例中，我们将下载并预处理 SMS Spam 文本数据。 然后，我们将使用 独热编码（one-hot-encoding）和 逻辑回归 。**\n",
    "\n",
    "**我们将使用这些独热矢量（one-hot-vectors）进行逻辑回归来预测文本是否为垃圾邮件（正常邮件标签：ham，垃圾邮件标签：spam）**\n",
    "\n",
    "**首先导入必要的库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**打开一个会话（session）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start a graph session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**检查数据是否已经下载，否则下载数据（作者给出的文件已经不存在了，现在已经变成SMSSpamCollection，所以我进行了修改）**\n",
    "\n",
    "**数据集链接：http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据集文件位置：./smsspamcollection/SMSSpamCollection\n",
    "save_file_name = os.path.join('smsspamcollection','SMSSpamCollection')\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists('smsspamcollection'):\n",
    "    print('Not Found ...')\n",
    "    os.makedirs('smsspamcollection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "messages = [line.rstrip() for line in open(save_file_name, 'r', encoding='UTF-8')]  # 没有encoding的话报错UnicodeDecodeError\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n"
     ]
    }
   ],
   "source": [
    "for message_no, message in enumerate(messages[:5]):\n",
    "    print(message_no, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**这是一个 TSV 文件（用制表符 tab 分隔），它的第一列是标记“正常信息”（ham）或“垃圾文件”（spam）的标签，第二列是信息本身**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham', 'ham', 'spam']\n",
      "5574\n",
      "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"]\n",
      "5574\n"
     ]
    }
   ],
   "source": [
    "text_label = []\n",
    "text_data = []\n",
    "for message in messages:\n",
    "    text = message.split('\\t')\n",
    "    text_label.append(text[0])\n",
    "    text_data.append(text[1])\n",
    "print(text_label[:3])\n",
    "print(len(text_label))\n",
    "print(text_data[:3])\n",
    "print(len(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对数据进行预处理，例如删除标点和数字等等.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relabel 'spam' as 1, 'ham' as 0\n",
    "target = [1 if x=='spam' else 0 for x in text_label]\n",
    "\n",
    "# Normalize text\n",
    "\n",
    "# Lower case\n",
    "texts = [x.lower() for x in text_data]\n",
    "\n",
    "# Remove punctuation and numbers\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "# Trim extra whitespace\n",
    "texts = [' '.join(x.split()) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry questionstd txt ratetcs apply overs',\n",
       " 'u dun say so early hor u c already then say',\n",
       " 'nah i dont think he goes to usf he lives around here though']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To determine a good sentence length to pad/crop at, we plot a histogram of text lengths (in words).**\n",
    "\n",
    "**为了确定填充/裁剪的良好句子长度，我们绘制文本长度的直方图（以单词表示）。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMht66lIsJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x288791b4048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Plot histogram of text lengths\n",
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We crop/pad all texts to be 25 words long.  We also will filter out any words that do not appear at least 3 times.**\n",
    "\n",
    "**我们将所有文本裁剪/填充为25个单词长度。我们还会过滤掉出现少于三次的单词。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose max text word length at 25\n",
    "sentence_size = 25\n",
    "min_word_freq = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow 有一个文本处理函数 `VocabularyProcessor()`. 我们使用这个函数来处理文本.**\n",
    "\n",
    "**tensorflow.contrib.learn.preprocessing.VocabularyProcessor(max_document_length, min_frequency = 0, vocabulary = None, tokenizer_fn = None)**\n",
    "\n",
    "参数：\n",
    "\n",
    "max_document_length: 文档的最大长度。如果文本的长度大于最大长度，那么它会被剪切，反之则用0填充\n",
    "\n",
    "min_frequency: 词频的最小值，出现次数小于最小词频则不会被收录到词表中\n",
    "\n",
    "vocabulary: CategoricalVocabulary 对象\n",
    "\n",
    "tokenizer_fn：分词函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup vocabulary processor\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "\n",
    "# Have to fit transform to get length of unique words.\n",
    "trans = vocab_processor.transform(texts)\n",
    "transformed_texts = np.array([x for x in trans])\n",
    "embedding_size = len(np.unique(transformed_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor object at 0x0000028873707F98>\n"
     ]
    }
   ],
   "source": [
    "print(vocab_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object VocabularyProcessor.transform at 0x00000288791A93B8>\n"
     ]
    }
   ],
   "source": [
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    2    3 ...,    0    0    0]\n",
      " [  21   22   23 ...,    0    0    0]\n",
      " [  27   28    8 ...,   44   45   46]\n",
      " ..., \n",
      " [8161  332    8 ...,    0    0    0]\n",
      " [ 141 2555  224 ...,   32  894   83]\n",
      " [3382   69 1250 ...,    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8165\n"
     ]
    }
   ],
   "source": [
    "print(embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**为了测试我们的 Logistic Model （预测邮件是spam还是ham），将数据集分成训练集和测试集。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split up data set into train/test\n",
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For one-hot-encoding, we setup an identity matrix for the TensorFlow embedding lookup.**\n",
    "\n",
    "**We also create the variables and placeholders for the logistic regression we will perform.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup Index Matrix for one-hot-encoding\n",
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))\n",
    "\n",
    "# Create variables for logistic regression\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# Initialize placeholders\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, we create the text-word embedding lookup with the prior identity matrix.**\n",
    "\n",
    "**Our logistic regression will use the counts of the words as the input.  The counts are created by summing the embedding output across the rows.**\n",
    "\n",
    "**Then we declare the logistic regression operations. Note that we do not wrap the logistic operations in the sigmoid function because this will be done in the loss function later on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text-Vocab Embedding\n",
    "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)\n",
    "\n",
    "# Declare model operations\n",
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we declare our loss function (which has the sigmoid built in), prediction operations, optimizer, and initialize the variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare loss function (Cross Entropy loss)\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "# Prediction operation\n",
    "prediction = tf.sigmoid(model_output)\n",
    "\n",
    "# Declare optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Intitialize Variables\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we loop through the iterations and fit the logistic regression on wether or not the text is spam or ham.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Over 4459 Sentences.\n",
      "Training Observation #10: Loss = 0.0554125\n",
      "Training Observation #20: Loss = 8.05668\n",
      "Training Observation #30: Loss = 0.0183169\n",
      "Training Observation #40: Loss = 0.26806\n",
      "Training Observation #50: Loss = 4.00304\n",
      "Training Observation #60: Loss = 3.10455\n",
      "Training Observation #70: Loss = 0.108278\n",
      "Training Observation #80: Loss = 0.0962993\n",
      "Training Observation #90: Loss = 0.37935\n",
      "Training Observation #100: Loss = 0.469175\n",
      "Training Observation #110: Loss = 0.0439357\n",
      "Training Observation #120: Loss = 0.339158\n",
      "Training Observation #130: Loss = 3.2729\n",
      "Training Observation #140: Loss = 0.21681\n",
      "Training Observation #150: Loss = 0.0467615\n",
      "Training Observation #160: Loss = 1.39431\n",
      "Training Observation #170: Loss = 0.0383452\n",
      "Training Observation #180: Loss = 0.00575125\n",
      "Training Observation #190: Loss = 0.000158529\n",
      "Training Observation #200: Loss = 9.18174e-05\n",
      "Training Observation #210: Loss = 0.00486015\n",
      "Training Observation #220: Loss = 1.67512\n",
      "Training Observation #230: Loss = 1.25798\n",
      "Training Observation #240: Loss = 2.58621e-05\n",
      "Training Observation #250: Loss = 8.96752\n",
      "Training Observation #260: Loss = 2.44423e-05\n",
      "Training Observation #270: Loss = 0.939406\n",
      "Training Observation #280: Loss = 0.0437929\n",
      "Training Observation #290: Loss = 0.00218007\n",
      "Training Observation #300: Loss = 7.74294\n",
      "Training Observation #310: Loss = 0.648034\n",
      "Training Observation #320: Loss = 0.00652748\n",
      "Training Observation #330: Loss = 6.86757e-05\n",
      "Training Observation #340: Loss = 0.00163358\n",
      "Training Observation #350: Loss = 0.0211147\n",
      "Training Observation #360: Loss = 0.218397\n",
      "Training Observation #370: Loss = 0.000105914\n",
      "Training Observation #380: Loss = 0.00915596\n",
      "Training Observation #390: Loss = 0.00957272\n",
      "Training Observation #400: Loss = 0.012276\n",
      "Training Observation #410: Loss = 0.0132357\n",
      "Training Observation #420: Loss = 0.753672\n",
      "Training Observation #430: Loss = 0.000432175\n",
      "Training Observation #440: Loss = 0.0274873\n",
      "Training Observation #450: Loss = 0.00065136\n",
      "Training Observation #460: Loss = 1.00153\n",
      "Training Observation #470: Loss = 0.0977955\n",
      "Training Observation #480: Loss = 7.96195\n",
      "Training Observation #490: Loss = 0.000166889\n",
      "Training Observation #500: Loss = 0.028575\n",
      "Training Observation #510: Loss = 0.0136832\n",
      "Training Observation #520: Loss = 0.0491223\n",
      "Training Observation #530: Loss = 5.83363\n",
      "Training Observation #540: Loss = 0.113553\n",
      "Training Observation #550: Loss = 0.0990502\n",
      "Training Observation #560: Loss = 0.116281\n",
      "Training Observation #570: Loss = 8.34364e-05\n",
      "Training Observation #580: Loss = 0.0129083\n",
      "Training Observation #590: Loss = 0.0963803\n",
      "Training Observation #600: Loss = 7.98149\n",
      "Training Observation #610: Loss = 4.24971\n",
      "Training Observation #620: Loss = 0.174475\n",
      "Training Observation #630: Loss = 3.8192\n",
      "Training Observation #640: Loss = 0.0202864\n",
      "Training Observation #650: Loss = 0.169607\n",
      "Training Observation #660: Loss = 0.0083322\n",
      "Training Observation #670: Loss = 2.41869\n",
      "Training Observation #680: Loss = 0.00141782\n",
      "Training Observation #690: Loss = 0.035201\n",
      "Training Observation #700: Loss = 0.00397747\n",
      "Training Observation #710: Loss = 0.121543\n",
      "Training Observation #720: Loss = 4.04153e-05\n",
      "Training Observation #730: Loss = 0.00223326\n",
      "Training Observation #740: Loss = 0.110372\n",
      "Training Observation #750: Loss = 0.0209645\n",
      "Training Observation #760: Loss = 2.21914\n",
      "Training Observation #770: Loss = 0.0169557\n",
      "Training Observation #780: Loss = 0.0634188\n",
      "Training Observation #790: Loss = 0.0137581\n",
      "Training Observation #800: Loss = 0.00515351\n",
      "Training Observation #810: Loss = 0.0624946\n",
      "Training Observation #820: Loss = 0.131183\n",
      "Training Observation #830: Loss = 0.000916459\n",
      "Training Observation #840: Loss = 0.00408524\n",
      "Training Observation #850: Loss = 8.37857e-05\n",
      "Training Observation #860: Loss = 0.00558356\n",
      "Training Observation #870: Loss = 0.0011686\n",
      "Training Observation #880: Loss = 0.0578552\n",
      "Training Observation #890: Loss = 0.00142075\n",
      "Training Observation #900: Loss = 0.00342246\n",
      "Training Observation #910: Loss = 0.0132792\n",
      "Training Observation #920: Loss = 0.0104731\n",
      "Training Observation #930: Loss = 0.111926\n",
      "Training Observation #940: Loss = 0.0199506\n",
      "Training Observation #950: Loss = 0.0062592\n",
      "Training Observation #960: Loss = 0.129314\n",
      "Training Observation #970: Loss = 0.553885\n",
      "Training Observation #980: Loss = 3.12076\n",
      "Training Observation #990: Loss = 0.000147132\n",
      "Training Observation #1000: Loss = 0.45019\n",
      "Training Observation #1010: Loss = 0.646045\n",
      "Training Observation #1020: Loss = 0.0315103\n",
      "Training Observation #1030: Loss = 0.0999217\n",
      "Training Observation #1040: Loss = 0.00943038\n",
      "Training Observation #1050: Loss = 0.0273565\n",
      "Training Observation #1060: Loss = 0.164555\n",
      "Training Observation #1070: Loss = 0.00526572\n",
      "Training Observation #1080: Loss = 0.000311195\n",
      "Training Observation #1090: Loss = 0.229663\n",
      "Training Observation #1100: Loss = 0.85142\n",
      "Training Observation #1110: Loss = 0.0200406\n",
      "Training Observation #1120: Loss = 0.0315016\n",
      "Training Observation #1130: Loss = 3.01856\n",
      "Training Observation #1140: Loss = 0.000366668\n",
      "Training Observation #1150: Loss = 0.232805\n",
      "Training Observation #1160: Loss = 0.0522258\n",
      "Training Observation #1170: Loss = 2.84999\n",
      "Training Observation #1180: Loss = 0.0401862\n",
      "Training Observation #1190: Loss = 0.0473211\n",
      "Training Observation #1200: Loss = 0.000354616\n",
      "Training Observation #1210: Loss = 0.0617873\n",
      "Training Observation #1220: Loss = 0.019608\n",
      "Training Observation #1230: Loss = 6.57898\n",
      "Training Observation #1240: Loss = 0.000994617\n",
      "Training Observation #1250: Loss = 1.50254e-05\n",
      "Training Observation #1260: Loss = 1.0259\n",
      "Training Observation #1270: Loss = 0.00273346\n",
      "Training Observation #1280: Loss = 0.0850693\n",
      "Training Observation #1290: Loss = 6.99885e-05\n",
      "Training Observation #1300: Loss = 0.0871392\n",
      "Training Observation #1310: Loss = 0.00705679\n",
      "Training Observation #1320: Loss = 0.0169119\n",
      "Training Observation #1330: Loss = 0.370601\n",
      "Training Observation #1340: Loss = 0.00402335\n",
      "Training Observation #1350: Loss = 0.000339839\n",
      "Training Observation #1360: Loss = 0.000300045\n",
      "Training Observation #1370: Loss = 0.019859\n",
      "Training Observation #1380: Loss = 2.41382e-05\n",
      "Training Observation #1390: Loss = 5.11197e-05\n",
      "Training Observation #1400: Loss = 5.15182e-05\n",
      "Training Observation #1410: Loss = 0.58206\n",
      "Training Observation #1420: Loss = 0.00548747\n",
      "Training Observation #1430: Loss = 0.236932\n",
      "Training Observation #1440: Loss = 5.60426e-05\n",
      "Training Observation #1450: Loss = 2.98084\n",
      "Training Observation #1460: Loss = 2.36357\n",
      "Training Observation #1470: Loss = 0.00046874\n",
      "Training Observation #1480: Loss = 0.889247\n",
      "Training Observation #1490: Loss = 2.14656\n",
      "Training Observation #1500: Loss = 0.00228321\n",
      "Training Observation #1510: Loss = 0.000743358\n",
      "Training Observation #1520: Loss = 0.00466523\n",
      "Training Observation #1530: Loss = 0.00850077\n",
      "Training Observation #1540: Loss = 0.168062\n",
      "Training Observation #1550: Loss = 0.398459\n",
      "Training Observation #1560: Loss = 0.0332462\n",
      "Training Observation #1570: Loss = 0.86392\n",
      "Training Observation #1580: Loss = 0.000484947\n",
      "Training Observation #1590: Loss = 0.00156246\n",
      "Training Observation #1600: Loss = 0.00488484\n",
      "Training Observation #1610: Loss = 2.11025\n",
      "Training Observation #1620: Loss = 0.0802304\n",
      "Training Observation #1630: Loss = 0.0056086\n",
      "Training Observation #1640: Loss = 9.93962e-05\n",
      "Training Observation #1650: Loss = 0.00418659\n",
      "Training Observation #1660: Loss = 0.000476705\n",
      "Training Observation #1670: Loss = 1.13492\n",
      "Training Observation #1680: Loss = 0.000848831\n",
      "Training Observation #1690: Loss = 0.172578\n",
      "Training Observation #1700: Loss = 0.0244438\n",
      "Training Observation #1710: Loss = 0.0538393\n",
      "Training Observation #1720: Loss = 0.000285141\n",
      "Training Observation #1730: Loss = 0.00449842\n",
      "Training Observation #1740: Loss = 0.0305337\n",
      "Training Observation #1750: Loss = 0.00558715\n",
      "Training Observation #1760: Loss = 0.00387305\n",
      "Training Observation #1770: Loss = 0.00181861\n",
      "Training Observation #1780: Loss = 0.0681061\n",
      "Training Observation #1790: Loss = 0.00196442\n",
      "Training Observation #1800: Loss = 1.01529\n",
      "Training Observation #1810: Loss = 3.91492\n",
      "Training Observation #1820: Loss = 0.00296944\n",
      "Training Observation #1830: Loss = 0.000302689\n",
      "Training Observation #1840: Loss = 1.70749\n",
      "Training Observation #1850: Loss = 0.000779894\n",
      "Training Observation #1860: Loss = 0.000376073\n",
      "Training Observation #1870: Loss = 0.0029314\n",
      "Training Observation #1880: Loss = 0.00273435\n",
      "Training Observation #1890: Loss = 3.80877\n",
      "Training Observation #1900: Loss = 0.00479789\n",
      "Training Observation #1910: Loss = 0.0109965\n",
      "Training Observation #1920: Loss = 0.000692936\n",
      "Training Observation #1930: Loss = 0.0498592\n",
      "Training Observation #1940: Loss = 0.000704239\n",
      "Training Observation #1950: Loss = 0.00223878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #1960: Loss = 0.00456574\n",
      "Training Observation #1970: Loss = 0.0966801\n",
      "Training Observation #1980: Loss = 3.2298\n",
      "Training Observation #1990: Loss = 0.00704119\n",
      "Training Observation #2000: Loss = 0.00139589\n",
      "Training Observation #2010: Loss = 0.000626952\n",
      "Training Observation #2020: Loss = 0.134017\n",
      "Training Observation #2030: Loss = 0.000421049\n",
      "Training Observation #2040: Loss = 0.00177466\n",
      "Training Observation #2050: Loss = 5.56001\n",
      "Training Observation #2060: Loss = 0.000680605\n",
      "Training Observation #2070: Loss = 2.50124e-05\n",
      "Training Observation #2080: Loss = 0.0234171\n",
      "Training Observation #2090: Loss = 8.05228\n",
      "Training Observation #2100: Loss = 0.00375684\n",
      "Training Observation #2110: Loss = 7.61789e-05\n",
      "Training Observation #2120: Loss = 0.0757297\n",
      "Training Observation #2130: Loss = 0.0222163\n",
      "Training Observation #2140: Loss = 6.61406e-05\n",
      "Training Observation #2150: Loss = 0.00370754\n",
      "Training Observation #2160: Loss = 0.0983844\n",
      "Training Observation #2170: Loss = 0.00194317\n",
      "Training Observation #2180: Loss = 0.0762728\n",
      "Training Observation #2190: Loss = 0.0313035\n",
      "Training Observation #2200: Loss = 0.00215511\n",
      "Training Observation #2210: Loss = 0.00831557\n",
      "Training Observation #2220: Loss = 0.0107504\n",
      "Training Observation #2230: Loss = 0.00177716\n",
      "Training Observation #2240: Loss = 0.000352467\n",
      "Training Observation #2250: Loss = 0.399048\n",
      "Training Observation #2260: Loss = 0.00333552\n",
      "Training Observation #2270: Loss = 0.000706679\n",
      "Training Observation #2280: Loss = 0.000274352\n",
      "Training Observation #2290: Loss = 0.00378623\n",
      "Training Observation #2300: Loss = 0.00261329\n",
      "Training Observation #2310: Loss = 0.00141679\n",
      "Training Observation #2320: Loss = 0.00769042\n",
      "Training Observation #2330: Loss = 1.28819e-05\n",
      "Training Observation #2340: Loss = 0.00462625\n",
      "Training Observation #2350: Loss = 0.00283813\n",
      "Training Observation #2360: Loss = 0.000230967\n",
      "Training Observation #2370: Loss = 0.000659269\n",
      "Training Observation #2380: Loss = 0.0662649\n",
      "Training Observation #2390: Loss = 0.00149587\n",
      "Training Observation #2400: Loss = 0.000505862\n",
      "Training Observation #2410: Loss = 13.2577\n",
      "Training Observation #2420: Loss = 0.0683197\n",
      "Training Observation #2430: Loss = 0.0100632\n",
      "Training Observation #2440: Loss = 0.149724\n",
      "Training Observation #2450: Loss = 0.00310117\n",
      "Training Observation #2460: Loss = 0.114182\n",
      "Training Observation #2470: Loss = 0.0782415\n",
      "Training Observation #2480: Loss = 0.00827823\n",
      "Training Observation #2490: Loss = 0.12922\n",
      "Training Observation #2500: Loss = 5.48105\n",
      "Training Observation #2510: Loss = 0.0013068\n",
      "Training Observation #2520: Loss = 0.521168\n",
      "Training Observation #2530: Loss = 0.342893\n",
      "Training Observation #2540: Loss = 0.00817697\n",
      "Training Observation #2550: Loss = 0.000399817\n",
      "Training Observation #2560: Loss = 0.00106175\n",
      "Training Observation #2570: Loss = 1.21523\n",
      "Training Observation #2580: Loss = 2.46791\n",
      "Training Observation #2590: Loss = 0.736919\n",
      "Training Observation #2600: Loss = 0.000145094\n",
      "Training Observation #2610: Loss = 0.000902883\n",
      "Training Observation #2620: Loss = 0.0055178\n",
      "Training Observation #2630: Loss = 0.0108178\n",
      "Training Observation #2640: Loss = 1.32045\n",
      "Training Observation #2650: Loss = 5.20213\n",
      "Training Observation #2660: Loss = 0.000180833\n",
      "Training Observation #2670: Loss = 0.013956\n",
      "Training Observation #2680: Loss = 0.0023373\n",
      "Training Observation #2690: Loss = 0.284123\n",
      "Training Observation #2700: Loss = 0.00574842\n",
      "Training Observation #2710: Loss = 1.65117\n",
      "Training Observation #2720: Loss = 3.2212\n",
      "Training Observation #2730: Loss = 0.000172686\n",
      "Training Observation #2740: Loss = 0.145566\n",
      "Training Observation #2750: Loss = 0.000256188\n",
      "Training Observation #2760: Loss = 0.0133885\n",
      "Training Observation #2770: Loss = 0.109063\n",
      "Training Observation #2780: Loss = 0.000133911\n",
      "Training Observation #2790: Loss = 5.2017\n",
      "Training Observation #2800: Loss = 8.46851\n",
      "Training Observation #2810: Loss = 0.000470763\n",
      "Training Observation #2820: Loss = 2.40723e-05\n",
      "Training Observation #2830: Loss = 0.00162183\n",
      "Training Observation #2840: Loss = 0.000709248\n",
      "Training Observation #2850: Loss = 0.00205562\n",
      "Training Observation #2860: Loss = 0.0061359\n",
      "Training Observation #2870: Loss = 0.007904\n",
      "Training Observation #2880: Loss = 0.00254679\n",
      "Training Observation #2890: Loss = 0.0214929\n",
      "Training Observation #2900: Loss = 0.067164\n",
      "Training Observation #2910: Loss = 0.00022196\n",
      "Training Observation #2920: Loss = 0.514576\n",
      "Training Observation #2930: Loss = 0.000726028\n",
      "Training Observation #2940: Loss = 0.0775136\n",
      "Training Observation #2950: Loss = 0.000764742\n",
      "Training Observation #2960: Loss = 0.00618711\n",
      "Training Observation #2970: Loss = 0.00254962\n",
      "Training Observation #2980: Loss = 0.0018686\n",
      "Training Observation #2990: Loss = 3.74723\n",
      "Training Observation #3000: Loss = 0.00138804\n",
      "Training Observation #3010: Loss = 0.0219704\n",
      "Training Observation #3020: Loss = 0.00124966\n",
      "Training Observation #3030: Loss = 5.5686\n",
      "Training Observation #3040: Loss = 3.27684\n",
      "Training Observation #3050: Loss = 0.22871\n",
      "Training Observation #3060: Loss = 0.114625\n",
      "Training Observation #3070: Loss = 0.000115314\n",
      "Training Observation #3080: Loss = 0.00016438\n",
      "Training Observation #3090: Loss = 1.33335\n",
      "Training Observation #3100: Loss = 6.86998e-05\n",
      "Training Observation #3110: Loss = 0.026454\n",
      "Training Observation #3120: Loss = 5.18896\n",
      "Training Observation #3130: Loss = 0.0191324\n",
      "Training Observation #3140: Loss = 2.45992\n",
      "Training Observation #3150: Loss = 0.125417\n",
      "Training Observation #3160: Loss = 0.00247296\n",
      "Training Observation #3170: Loss = 4.72864e-06\n",
      "Training Observation #3180: Loss = 0.0736599\n",
      "Training Observation #3190: Loss = 0.100037\n",
      "Training Observation #3200: Loss = 0.00763895\n",
      "Training Observation #3210: Loss = 2.86473\n",
      "Training Observation #3220: Loss = 0.0150359\n",
      "Training Observation #3230: Loss = 0.013015\n",
      "Training Observation #3240: Loss = 0.248964\n",
      "Training Observation #3250: Loss = 0.0014758\n",
      "Training Observation #3260: Loss = 0.000283315\n",
      "Training Observation #3270: Loss = 0.0215165\n",
      "Training Observation #3280: Loss = 3.78378\n",
      "Training Observation #3290: Loss = 0.0602881\n",
      "Training Observation #3300: Loss = 0.00112295\n",
      "Training Observation #3310: Loss = 0.00151147\n",
      "Training Observation #3320: Loss = 3.95757\n",
      "Training Observation #3330: Loss = 0.000220211\n",
      "Training Observation #3340: Loss = 6.25769\n",
      "Training Observation #3350: Loss = 0.011247\n",
      "Training Observation #3360: Loss = 7.11429e-05\n",
      "Training Observation #3370: Loss = 0.00540771\n",
      "Training Observation #3380: Loss = 0.0037667\n",
      "Training Observation #3390: Loss = 0.00362309\n",
      "Training Observation #3400: Loss = 9.39061e-06\n",
      "Training Observation #3410: Loss = 0.00047758\n",
      "Training Observation #3420: Loss = 0.0736549\n",
      "Training Observation #3430: Loss = 0.175065\n",
      "Training Observation #3440: Loss = 0.000637519\n",
      "Training Observation #3450: Loss = 0.000654312\n",
      "Training Observation #3460: Loss = 0.00100462\n",
      "Training Observation #3470: Loss = 0.00252031\n",
      "Training Observation #3480: Loss = 0.000434103\n",
      "Training Observation #3490: Loss = 0.000485539\n",
      "Training Observation #3500: Loss = 2.67945e-05\n",
      "Training Observation #3510: Loss = 5.79625\n",
      "Training Observation #3520: Loss = 0.000231258\n",
      "Training Observation #3530: Loss = 0.0963266\n",
      "Training Observation #3540: Loss = 0.000876526\n",
      "Training Observation #3550: Loss = 0.0189324\n",
      "Training Observation #3560: Loss = 0.000185787\n",
      "Training Observation #3570: Loss = 0.0235251\n",
      "Training Observation #3580: Loss = 0.0261896\n",
      "Training Observation #3590: Loss = 8.14001e-06\n",
      "Training Observation #3600: Loss = 0.0410238\n",
      "Training Observation #3610: Loss = 0.248675\n",
      "Training Observation #3620: Loss = 0.776187\n",
      "Training Observation #3630: Loss = 0.00484389\n",
      "Training Observation #3640: Loss = 0.158592\n",
      "Training Observation #3650: Loss = 0.636528\n",
      "Training Observation #3660: Loss = 0.000902516\n",
      "Training Observation #3670: Loss = 0.109428\n",
      "Training Observation #3680: Loss = 0.000253745\n",
      "Training Observation #3690: Loss = 0.00222018\n",
      "Training Observation #3700: Loss = 0.00356816\n",
      "Training Observation #3710: Loss = 0.00155236\n",
      "Training Observation #3720: Loss = 1.13628e-05\n",
      "Training Observation #3730: Loss = 0.136686\n",
      "Training Observation #3740: Loss = 0.0171761\n",
      "Training Observation #3750: Loss = 0.00126618\n",
      "Training Observation #3760: Loss = 6.63792e-05\n",
      "Training Observation #3770: Loss = 2.0885\n",
      "Training Observation #3780: Loss = 0.000220367\n",
      "Training Observation #3790: Loss = 0.10265\n",
      "Training Observation #3800: Loss = 0.0109049\n",
      "Training Observation #3810: Loss = 4.15981\n",
      "Training Observation #3820: Loss = 0.0363126\n",
      "Training Observation #3830: Loss = 0.000217195\n",
      "Training Observation #3840: Loss = 1.04578\n",
      "Training Observation #3850: Loss = 0.00403554\n",
      "Training Observation #3860: Loss = 1.04915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #3870: Loss = 0.0162772\n",
      "Training Observation #3880: Loss = 0.12118\n",
      "Training Observation #3890: Loss = 0.00357007\n",
      "Training Observation #3900: Loss = 0.000945435\n",
      "Training Observation #3910: Loss = 0.000169133\n",
      "Training Observation #3920: Loss = 0.109346\n",
      "Training Observation #3930: Loss = 0.0177977\n",
      "Training Observation #3940: Loss = 2.75338e-05\n",
      "Training Observation #3950: Loss = 3.42797e-05\n",
      "Training Observation #3960: Loss = 0.00020922\n",
      "Training Observation #3970: Loss = 0.0157473\n",
      "Training Observation #3980: Loss = 3.27938\n",
      "Training Observation #3990: Loss = 0.000290787\n",
      "Training Observation #4000: Loss = 0.000202368\n",
      "Training Observation #4010: Loss = 0.00655289\n",
      "Training Observation #4020: Loss = 3.70055\n",
      "Training Observation #4030: Loss = 0.0152539\n",
      "Training Observation #4040: Loss = 0.000457979\n",
      "Training Observation #4050: Loss = 0.00304603\n",
      "Training Observation #4060: Loss = 0.000583809\n",
      "Training Observation #4070: Loss = 0.00998161\n",
      "Training Observation #4080: Loss = 0.000655216\n",
      "Training Observation #4090: Loss = 4.88401\n",
      "Training Observation #4100: Loss = 0.000777877\n",
      "Training Observation #4110: Loss = 0.0021084\n",
      "Training Observation #4120: Loss = 1.89074\n",
      "Training Observation #4130: Loss = 0.00772465\n",
      "Training Observation #4140: Loss = 0.000856119\n",
      "Training Observation #4150: Loss = 0.00224662\n",
      "Training Observation #4160: Loss = 0.00104855\n",
      "Training Observation #4170: Loss = 6.19368\n",
      "Training Observation #4180: Loss = 1.64915\n",
      "Training Observation #4190: Loss = 1.16766\n",
      "Training Observation #4200: Loss = 0.0193811\n",
      "Training Observation #4210: Loss = 0.537943\n",
      "Training Observation #4220: Loss = 0.00201919\n",
      "Training Observation #4230: Loss = 1.01594e-05\n",
      "Training Observation #4240: Loss = 1.50938e-05\n",
      "Training Observation #4250: Loss = 0.000265305\n",
      "Training Observation #4260: Loss = 0.0034291\n",
      "Training Observation #4270: Loss = 0.00189679\n",
      "Training Observation #4280: Loss = 0.000112182\n",
      "Training Observation #4290: Loss = 0.0361652\n",
      "Training Observation #4300: Loss = 1.65273\n",
      "Training Observation #4310: Loss = 0.0134171\n",
      "Training Observation #4320: Loss = 0.00152636\n",
      "Training Observation #4330: Loss = 0.000469055\n",
      "Training Observation #4340: Loss = 0.0473129\n",
      "Training Observation #4350: Loss = 0.0950761\n",
      "Training Observation #4360: Loss = 0.000781797\n",
      "Training Observation #4370: Loss = 5.71211\n",
      "Training Observation #4380: Loss = 0.00262726\n",
      "Training Observation #4390: Loss = 0.00560893\n",
      "Training Observation #4400: Loss = 0.000693763\n",
      "Training Observation #4410: Loss = 0.000107642\n",
      "Training Observation #4420: Loss = 5.46867\n",
      "Training Observation #4430: Loss = 0.00129601\n",
      "Training Observation #4440: Loss = 0.0126473\n",
      "Training Observation #4450: Loss = 0.00311209\n"
     ]
    }
   ],
   "source": [
    "# Start Logistic Regression\n",
    "print('Starting Training Over {} Sentences.'.format(len(texts_train)))\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    \n",
    "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    if (ix+1)%10 == 0:  # 每隔10输出一次loss值\n",
    "        print('Training Observation #' + str(ix+1) + ': Loss = ' + str(temp_loss))\n",
    "        \n",
    "    # Keep trailing average of past 50 observations accuracy\n",
    "    \n",
    "    # Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    \n",
    "    # Get True/False if prediction is accurate\n",
    "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    if len(train_acc_all) >= 50:\n",
    "        train_acc_avg.append(np.mean(train_acc_all[-50:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练好 logistic 模型以后，就可以在测试集上运行模型得到准确率。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Test Set Accuracy For 1115 Sentences.\n",
      "Test Observation #50\n",
      "Test Observation #100\n",
      "Test Observation #150\n",
      "Test Observation #200\n",
      "Test Observation #250\n",
      "Test Observation #300\n",
      "Test Observation #350\n",
      "Test Observation #400\n",
      "Test Observation #450\n",
      "Test Observation #500\n",
      "Test Observation #550\n",
      "Test Observation #600\n",
      "Test Observation #650\n",
      "Test Observation #700\n",
      "Test Observation #750\n",
      "Test Observation #800\n",
      "Test Observation #850\n",
      "Test Observation #900\n",
      "Test Observation #950\n",
      "Test Observation #1000\n",
      "Test Observation #1050\n",
      "Test Observation #1100\n",
      "\n",
      "Overall Test Accuracy: 0.7964125560538117\n"
     ]
    }
   ],
   "source": [
    "# Get test set accuracy\n",
    "print('Getting Test Set Accuracy For {} Sentences.'.format(len(texts_test)))\n",
    "test_acc_all = []\n",
    "\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_test)):\n",
    "    y_data = [[target_test[ix]]]\n",
    "    \n",
    "    if (ix+1)%50 == 0:  # 每隔50输出一次\n",
    "        print('Test Observation #' + str(ix+1))\n",
    "    \n",
    "    # Get prediction of single observation\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    \n",
    "    # Get True/False if prediction is accurate\n",
    "    test_acc_temp = target_test[ix]==np.round(temp_pred)\n",
    "    test_acc_all.append(test_acc_temp)\n",
    "\n",
    "print('\\nOverall Test Accuracy: {}'.format(np.mean(test_acc_all)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at the training accuracy over all the iterations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28879bb2c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Plot training accuracy over time\n",
    "plt.plot(range(len(train_acc_avg)), train_acc_avg, 'k-', label='Train Accuracy')\n",
    "plt.title('Avg Training Acc Over Past 50 Iterations')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**限制句子（或文本）大小的原因值得在这里解释一下。在此示例中，我们将文本大小限制为25个单词。这是 \"bag of words\" 的常见做法，因为它限制了文本长度对预测的影响。你可以想象，如果我们找到一个单词，例如“会议 (meeting) ”，这可以预测文本是ham（而不是spam），那么垃圾邮件很可能会通过在邮件中多次输入该单词来避开垃圾邮件的检测。实际上，这是目标数据不平衡 (imbalanced target data) 的常见问题。在这种情况下很可能会出现不平衡的数据，因为垃圾邮件 (spam) 可能很难找到，而正常的邮件 (ham) 可能很容易找到。由于这个事实，我们创建的词表可能会严重偏向我们数据的ham部分所代表的单词（更多的 ham 意味着更多的单词在ham中表示而不是 spam ）。如果我们允许长度不限的文本，那么垃圾邮件制造者可能会利用这一点并创建非常长的文本，这些文本在我们的模型中触发正常邮件 word factors 的概率更高。**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
